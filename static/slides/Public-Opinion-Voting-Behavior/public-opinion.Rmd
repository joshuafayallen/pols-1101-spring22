---
title: "Chapter 9"
subtitle: "Public Opinion"
author: "Josh Allen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: ["default"]
    self_containted: true
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: true
---

```{r preamble, child = here::here('preamble.Rmd')}
```

---

## Looming Deadlines 

- Extra Credit for completing NCCHR early .bold[02/27]

- Chapters

---


---
class: center, middle

# Public Opinion
--

# The Stats Part

---

---
class: center, middle

<gsu-blockquote-red>
Itt seems to me that statistics is often sold as a sort
of alchemy that transmutes randomness into certainty, an “uncertainty laundering” that begins with data and concludes with success as measured by statistical significance - Andrew Gelman(2016)
</gsu-blockquote-red>

???
A part of the book that I want to expand upon is how you all think about the stats you consume during election cycles. We are about to see a lot more of it as we go into the midterm elections. How we get to the numbers that talented statisticians get to is not always obvious. So the goal of this portion of the lecture is to demistify statistics a bit in this lecture.
---

---

## The Basics 

- .bold[Population]: all the cases of interest in existence, all of something, group, large or small, that we are interested in

- .bold[Sample]: a selected subset of the population from which data is collected for study

- .bold[Parameter]: measurements of population quantities

- .bold[Statistics]: measurements of the sample quantities are called

--

- .bod[Inferential Statistics]: we try to make inferences about true population parameter values based on statistics. Make predictions about population parameters based on information derived from a sample.

- .bold[Descriptive Statistics]: are used to provide a summary of the information in a dataset. E.g., mean, median, variance

???
The book tries to skirt around "scary concepts like these". In public opinion we are trying to understand what the broader population of the United States feels like but it is a bit more difficult than asking each and every person what they think about the something. In the next few classes we will cover why that is. What is important to keep in mind that the book doesn't talk about is that it covers one very widely used paradigm very poorly. The paradigm that it uses is what is known as frequentism. In frequentism we assume that the sample is random but the world is fixed. The population parameter in frequentist statistics is essentially an unknowable thing that we are trying to figure out. 
---

---
## Basics Continued

-  .bold[Probability]: is the proportion of times that the outcome would occur in a very long sequence of observations

- .italic[p]-value: is the probability that we would get our sample value given that the null hypothesis is true. 

- .bold[Confidence-Interval]: the probability that a population parameter will fall between a set of values for a certain proportion of times. 

???
These concept s are depeely interconnected not only to how we conduct statistics in polling but to each other. The p-value is a threshold that we apply all the time in when doing statististics in political science, economics, sociology, psychology, epidemiology, education, and biostatistics literature. The p-value is a threshold that we often use to set a minimal standard that there is a relationship between our variables. The confidence interval is just essentially a net. What ever quanitity of interst we are calculating there is going to be some error in real life analysis that will make our sample quantity differ from the population quanitity. The key question is whether or not that deviation is so extreme that we aren't actually capturing the population parameter.

The problem with the statistics we normally used and what is widely taught is kind of encapsulated for bu the confidence interval. What do you think that the plain english interpretation of the confidence interval is? 


The problem is that the confidence interval is us saying that 95% of the time we believe that our net captures the true population parameter. The stats we use do not really track with how we think or in practice what we are doing. 
---

---
## Confidence Intervals in Practice

```{r}
p <- 0.45
N <- 1000
B <- 10000
inside <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
})

tab <- replicate(100, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  hit <- between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
  c(x_hat, x_hat - 1.96 * se_hat, x_hat + 2 * se_hat, hit)
})


tab <- data.frame(poll=1:ncol(tab), t(tab))
names(tab)<-c("poll", "estimate", "low", "high", "hit")
tab <- mutate(tab, p_inside = ifelse(hit, "Yes", "No") )

ggplot(tab, aes(poll, estimate, ymin=low, ymax=high, color = p_inside)) + 
  geom_point()+
  geom_errorbar() + 
  coord_flip() + 
  geom_hline(yintercept = p) +
  theme_allen() +
  guides(color = guide_legend("Parameter Inside"))

```


???
You are going to see a lot of simulations because that is what I find most intuitive. Here is what is going on. I set a value for the population parameter and then took random samples from that population. Than what i did was calculte the 95% confidence intervals. About 95% of the time I captre the true value with my net. There are a few times where I don't. Effectively when polls report margin of errors they are in effect reporting a confidence interval

---




---
## Probability 

.pull-left[

-	We think of probability is indicated as number between 0 (never happens) and 1 (always happened) repeated over an infinite number of times

- The probability of picking a certain color candy from a jar given a set of candies in the jar

- Lets say we have 2 red candies and 3 blue candies and we want to know the probability of picking a red one. 
]

--


.pull-right[

```{r echo = TRUE}

set.seed(1994)

candies = rep(c("red","blue"), times = c(2,3))

candies 



```

]


???
This is pretty easy! it is just 2/5 or 40% and this should be true as we pick candies. So lets test this out with some simulations! here I am just setting up our jar with the situation that I outlined and checking to make sure that it is set up correctly
---

---
layout: false

.pull-left[
### Sampling Once
```{r echo = TRUE}
sample(candies, 1)
```



]

.pull-right[
### Sampling 100,000 Times

```{r echo =TRUE}

b = 100000

events = replicate(b, sample(candies),1)

tab = table(events)


prop.table(tab )


```



]
???
This is pretty neat! with our simple math we can show this holds in our toy problem! However this can start to change if the events are not independent of each other. Think of a deck of card. Where there are 52 cards with 4 face cards for each suit. As the dealer deals out cards the probabilty of you getting a face card goes down
---

---
## Monty Hall

```{r}
vembedr::embed_url("https://youtu.be/QGxyIQzLeUc")
```




---


---
## So Who Was Right

.pull-left[
```{r echo = TRUE}
B = 10000
monty_hall = function(strategy){
  doors = as.character(1:3)
  prize = sample(c("car", "goat", "goat"))
  prize_door = doors[prize == "car"]
  my_pick  = sample(doors, 1)
  show = sample(doors[!doors %in% c(my_pick, prize_door)],1)
  stick = my_pick
  stick == prize_door
  switch = doors[!doors%in%c(my_pick, show)]
  choice = ifelse(strategy == "stick", stick, switch)
  choice == prize_door
}

```


]

--

```{r echo = TRUE}
stick = replicate(B, monty_hall("stick"))
mean(stick)

switch = replicate(B, monty_hall("switch"))

mean(switch)

```

???
In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?

We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3!
 
Monty hall is a good demonstration that we don't always have a good intuition on the probability of things 

I thought this was important to include because IDK if you'll ever take a class with me again so I thought I would pass on some advice on how to win a game show using probability theory 
---





---
## Polling 

.pull-left[
<gsu-blockquote-red>

Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy

</gsu-blockquote-red>
]

--


.pull-right[
<gsu-blockquote-blue>
Good pools are all alike; every bad poll is bad in its own way.” –– Professor Allen
</gsu-blockquote-blue>
]
???
This is a bit of an embellisment on my part but good polls rely on what is referred to random sampling. If we think back to our candy jar each candy has a relatively equal probability of getting picked. Good polls adopt strategies that ensure that we can get a representative sample of the different kinds of people. Good polls employ questions that are valid meaning they are measuring what we think they are meausring and are reliable meaning they produce the same response 


---


---
## Bad Polls 

- .bold[Leading Questions]: encourage the respondent to give a certain answer

- .bold[Double Barrelled Questions]: Ask for opinions about more than one issue

- .bold[Social Desirability Bias]: Frames questions in a way that it makes one believe tthat a certain answer is not preferred

- .bold[Priming]: a previous question which influences answers to future questions

???
We find some some subset of problems in polling that lead us toward problems whether they are intentional or unitentional 

---

---
## How Questions are Worded


.pull-left[

### Free Speach
<gsu-blockquote-alt>
Considering the importance of the First Amendment right to free speech, do you think the KKK should be allowed to express their opinions freely at public events?

</gsu-blockquote-alt>
]

.pull-right[
### Public Safety

<gsu-blockquote-red>

Considering the fact that KKK rallies often lead to violent outbursts, do you think the KKK should be allowed to express their opinions freely at public events?


</gsu-blockquote-red>

]


???
These are examples of Framing Effects.

Definition: When people react to a particular choice in different ways depending on how information is presented. 

These kinds of questions can really affect what we can actually say about our quantity of interest. However just as critically how we get our sample matters

---

---
## Literary Digest vs. George Gallup


```{r}
vembedr::embed_url("https://www.youtube.com/watch?v=R2vhjC5qCQk")
```


???
Would you charecterize asking what your friends think about the president a good reperesentation of the feelings of president writ large? Probably not but that is kind of the literay digest poll did 

---



---
## What Happened...Kind of 


```{r}
library(ggdag)
nice_skill_data = tibble(height = rnorm(10000, mean = 0, sd = 1),
                               skill = rnorm(10000, mean = 0, sd = 1))
plot_labels = tribble(
  ~x, ~y, ~label, 
  -2, 2, "Short and Skilled",
  2, 2, "Tall and Skilled",
  2, -2, "Tall and Not Skilled",
  -2, -2, "Short and Not SKilled",
)
full_sample = ggplot(nice_skill_data, aes(x = height, y = skill)) +
  geom_point(color = "grey40", size = 0.5, alpha = 0.8) +
  geom_smooth(size = 2, method = "lm") +
  labs(x = "height", y = "Number of Points Scored") +
  theme_bw(base_family = "Lato") +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())


full_sample_annotated = full_sample +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_label(data = plot_labels, aes(label = label, x = x, y = y),
             fill = "#CF4446", color = "white",
             family = "Lato", fontface = "bold") 




collider_bias = ggplot(filter(nice_skill_data, 
                               !(height <= 0 & skill <= 0)), 
       aes(x = height, y = skill)) +
  geom_point(color = "grey40", size = 0.5, alpha = 0.8) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_smooth(size = 2, method = "lm") +
  geom_label(data = plot_labels, aes(label = label, x = x, y = y),
             fill = "#CF4446", color = "white",
             family = "Lato", fontface = "bold") +
  labs(x = "height", y = "Points") +
  theme_bw(base_family = "Lato")





ggsave(full_sample, filename = "collider_bias.png", dpi = 500)



collider_dag = dagify(
  Y ~ X,
  Z ~ X + Y,
  coords = list(x = c(X = 1, Y = 3, Z = 2),
                y = c(X = 1, Y = 1, Z = 2)),
  exposure = "X",
  outcome = "Y"
) %>% 
  tidy_dagitty() %>% 
  node_status() %>% 
  mutate(across(c(name, to), 
                ~recode(., X = "height", Y = "Points Scored", 
                        Z = "Being in the NBA"))) %>% 
  mutate(arrow_color = ifelse(to == "Being in the NBA", "#F012BE", "black"))
plot_dag = ggplot(collider_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_edges(aes(edge_color = arrow_color)) +
  geom_dag_point(aes(color = status), size = 17) +
  geom_dag_text(data = filter(collider_dag, name != "Being in the NBA"),
                color = "black", size = 5, nudge_y = -0.2,
                family = "Lato", fontface = "bold") +
  geom_dag_text(data = filter(collider_dag, name == "Being in the NBA"),
                color = "black", size = 5, nudge_y = 0.2,
                family = "Lato", fontface = "bold") +
  scale_color_manual(values = c("#2ECC40", "#0074D9"), 
                     na.value = "grey80", guide = "none") +
  coord_cartesian(xlim = c(0.8, 3.2), ylim = c(0.8, 2.2)) +
  theme_dag()


full_sample

```


???
Intuitively this kind of makes sense why the readers digest poll did really poorly because they conditioned on what statisticians call a collider. Let us take an intuitive example of why doing this is bad. Would we think that being taller help you score more point in a basketball game. I learned it like this and throught the example of being nice and attractiveness 
---


---
## What Happened... Kind of 
.pull-left[
```{r  fig.width=4.5, fig.height=3.35, out.width="100%"}


plot_dag
```
]


.pull-right[
```{r  fig.width=4.5, fig.height=3.35, out.width="100%"}

full_sample_annotated
```

]
???
In our toy model what I did was basically included being in the NBA in the model distoring the true effect of 

---

---
## What.. Happened Kind of 

```{r}
collider_bias
```


???
This is what the relationship looks like in real life. 
---


---
## Normal Distributions 

```{r}
pacman::p_load("stevemisc", "dqrng")
ggplot(data.frame(x = c(-4, 4)), aes(x)) +
         stat_function(fun = dnorm, color = "#0039A6", size = 1.5) +
  labs(title = "A Simple Normal Density Function",
       subtitle = "The mean parameter determines the central tendency\n and standard deviation parameter determines the width.",
       x = "", y="") + 
  theme_allen()



```


???
The coolest part about when you do sampling well is that we can approximate the population parameter. There are lots of sampling distributions but today we will cover the what is called the normal distribution or the Gaussian distribution. The normal distribution is defined by the mean and standard deviation. The mean sets the center of the distribution and the standard deviation 



---


---
## Central Limit Theorum


```{r }


normal_dist("#522d80","#F66733", "Open Sans") + 
  theme_steve_web() + post_bg() +
  # ^ all from stevemisc
    labs(title = "The Area Underneath a Normal Distribution",
       y = "Density",
       x = "")
```


???
If you have taken basic stats you know that the spread of the distribution can be described like this 



---


---
## The Central Limit Theorem


```{r}

 load("data/therms18.rda")

Therms18 %>%
  group_by(fttrump) %>%
  tally() %>% 
  ggplot(.,aes(fttrump, n)) + geom_bar(stat="identity", fill="#619cff",color="black") +
  theme_allen() + 
  scale_x_continuous(breaks = seq(0, 100, by=10)) +
  labs(x = "Thermometer Rating for Donald Trump",
       y = "Number of Observations with This Particular Rating",
       caption = "Data: ANES Pilot Study, 2018. Number of observations: 2,471.",
       title = "The Thermometer Ratings for Donald Trump")

```



??? 
What is cool about the central limit theorum is that if we keep retaking samples of the larger population throught random sampling the sample means will not only be normally distributed but we would converge to the population mean and the random sampling error would equal the standard error of the sample means. Even this one! This is just a therometer rating where people either really like trump in 2018 or really do not like him. This likely hold true for Biden as well but I didn't want to pull and clean the latest ANES
---


---
layout: false 


.pull-left[

```{r echo = TRUE}
Population = rbnorm(250000, mean =40.01578, sd = 40.24403,
                     lowerbound = 0, 
                     upperbound = 100,
                     round = TRUE,
                     seed = 1994)



Popsamples = tibble(
  samplemean=sapply(1:1000000, 
           function(i){ x = mean(
             dqsample(Population, 10, 
                    replace = FALSE)) 
           })) 

```

]

.pull-right[
```{r}


Popsamples %>%
  ggplot(.,aes(samplemean)) + geom_histogram(binwidth=.5,aes(y=..density..),alpha=0.7) +
  theme_steve_web() + post_bg() +
  geom_vline(xintercept = mean(Population), linetype="dashed") +
  stat_function(fun=dnorm,
                color="#522d80", size=1.5,
                args=list(mean=mean(Popsamples$samplemean), 
                          sd=sd(Popsamples$samplemean))) +
  labs(x = "Sample Mean", y = "Density",
       title = "The Distribution of 1,000,000 Sample Means",
       subtitle = "Notice the distribution is normal and the mean of sample means converges on the \nknown population mean (vertical line).",
       caption = "Data and Code from svmiller.com")
```
]
???
We are going to do this by just making up some data, setting the sample mean equal to 40 and a standard deviation of 40. Than we are going to run this a million times! Each sample size is only 10 which generally will invite trouble if you only do it once. Put since we are doing it a million times the central limit will kick in and help us out.

This is honestly still super cool to me. 

---

---
## Why Should You care

- In practice  we can't ever do this 

- Samples are very expensive

- Attitudes change so often

- Sampling Error

???
Polls are insanely expensive to run and we never know the "truth" so this brings us to sampling error. The cost of doing this work is that we are bound to 

of trying to infer properties of the population given a sample of it. Indeed, random sampling purposely introduces random sampling error to our estimate since there will always be differences in the sample from the population that occur just by the chance sample we obtained. This may seem bad, but statisticians and social scientists will note that random sampling error is always the lesser evil to systematic error

We conceptualize random sampling error as having two components. The first is the amount of variation in the population parameter. We can’t do anything about this. Real world data can be noisy as hell, like the thermometer ratings for divisive public officials in the United States. The second component of random sampling error is the sample size component. We can do something about this: increase the number of observations in the sample. 

---


---
## Sample Sizes 

```{r echo = TRUE}
sample_sizes = c(10, 25, 100, 400, 1000, 2000)

Samps = list() 
set.seed(1994)
for (j in sample_sizes) {
   Samps[[paste0("Sample size: ", j)]] = data.frame(sampsize=j, samp=sapply(1:10, function(i){ x = sample(Population, j, replace = TRUE) }))
}

Samps = Samps %>%
  map_df(as_tibble) %>%
  gather(samp, value, samp.1:samp.10)



```


---

---
layout:false



```{r}
Samps %>%
  group_by(sampsize, samp) %>%
  summarize(sampmean = mean(value)) %>%
  ggplot(., aes(as.factor(sampsize),sampmean)) + 
  geom_point(size=3, color="black", alpha=0.5) +
  theme_allen() +
  geom_hline(yintercept = mean(Population), linetype="dashed") +
  labs(x = "Sample Size",
       y = "Sample Means",
       title = "Ten Sample Means of Varying Sample Sizes from a Population",
       subtitle = "The diminishing returns of increasing sample size emerge around 1,000 observations, even as the spread in these simulated data is quite large.",
       caption = "Data: Simulated data from a blog post at svmiller.com")
```



???
With this in mind, what is an ideal sample size in a situation like this (and likely other cases in a social scientific application) where there is high variation in the population and infinity trials—or even multiple trials—are not possible? In the case of the Population data we simulated above, let’s get 10 sample means from samples of varying sizes: 10, 25, 100, 400, 1000, 2000, 4000, and 10000 from this population of 250,000 observations. The following code will both execute that code and chart the 10 different sample means for each different sample size noted on the x-axis.

---


---
## What Does that Buy us?


- We calculate what are known as standard errors by $SE = \frac{\sigma}{\sqrt{n}}$

- This affects our confidence because we calculate it $CI = \bar{x}\pm z \frac{\sigma}{\sqrt{n}}$

???
If you remember back to the first few slides we talked about confidence intervals/Margins of error. We know that a lot of the time that we can't always capture the population parameter but a lot of the time our net will one way to sure that net is tighter is to increase our sample. Our standard deviations are calculated by taking the sqrt of our sample size. our confidence interval than is just adding or subtracting a critical value typically 1.96. Notice that N appears in the denominator. This will shrink our confidence intervals! As we divide a small number by a super big number than we get a small number 
---

---
layout:false 

```{r}
Samps %>%
  group_by(sampsize, samp) %>%
  mutate(sampmean = mean(value),
         se = sd(Population)/sqrt((sampsize)),
         lb95 = sampmean - 1.96*se,
         ub95 = sampmean + 1.96*se) %>%
  distinct(sampsize, samp, sampmean, se, lb95, ub95) %>%
  ungroup() %>%
  mutate(sampsize = fct_inorder(paste0("Sample Size: ", sampsize)),
         samp = as.numeric(str_replace(samp, "samp.", ""))) %>%
  ggplot(.,aes(as.factor(samp), sampmean, ymax=ub95, ymin=lb95)) +
  theme_steve_web() + post_bg() + 
  facet_wrap(~sampsize) +
  geom_hline(yintercept = mean(Population), linetype="dashed") +
  geom_pointrange() + coord_flip() +
  labs(y = "Sample Mean (with 95% Intervals)",
       x = "Sample Number [1:10]",
       title = "Ten Sample Means of Varying Sizes (with 95% Intervals) from a Population")
```



---

---
## Rough Heuristics

- Consider possible motives of the poll organizers

- Examine how questions are worded 

- Check the target population and how it is sampled

- Asess the sample size and margin of error 

- Compare across survey houses and look at poll aggregators 

???
These are about as rough of heuristics as you can come by. As we have seen sample size doesnt neccessarily get you anywhere if the poll is junk. A junk poll with huge sample sizes will naturally have tigher margins of error, but not actually get you a good indicator of public opinion

---

---
class: center, middle, inverse

# Public Opinion

--

# Non-Stats Part
---

---
## What is Public Opinion?

We generally think of it as politically relevant opinions held by citizens that are expressed openly

--

- .bold[Political Values]: The basic principles that people hold about government 

- .bold[Attitude]: The predisposition to act on a particular issue

- .bold[Opinion]: Expression or judgement on a particular issue 


---



---
## Political Socialization 

<gsu-blockquote-alt> 

The process through which individuals and citizens acquire their political opinions

</gsu-blockquote-alt> 


???
The process through which individuals and citizens acquire their political opinions

This is a learned effect. You ARE NOT born with a political identity!


Primacy vs. Cumulative Effects
Primacy Effect – the phenomenon of aging and growth in which the ideas that an individual learns at a young age (primacy) are more likely to stick
Cumulative Effect – the phenomenon of political socialization in which an individual’s political orientation will become more firm or solidified with age

---

---
## Agents of Socialization 

Most socialization occurs before adulthood

By the time you’re done with school, you’ll likely already know which party you affiliate with
Primary agents of political socialization:
  - Family
  - School
  - Peers

What people and how people think about politics is closely related to:
  - Social class
  - Religion
  - Ethnicity 
  - Gender
  - Race

???


---


---
## Agents of Socialization 
### The Family

Children tend to adopt the political party of their parents 

People quickly become attached to their “team” and sometimes don’t think about why they like that “team”


.bold[Political efficacy]: the degree to which one feels they can make a difference in politics
It’s important to remember that social class plays a strong role in socialization

???

Party affliation kind of works in a similar manner. If everybody you know and love is a 49ers fan that how much sense is it going to make for you to go out and start liking the Cowboys or the Seahawks

A similar logic underlies our politial socialization. We learn how to deal with authority from our parents and these attitudes can impact later beliefs on how we navigate our relation to power


similarly the premium we placee on our own agency to affect change is affected by our parents. Often times rich people beleive that they can make a difference in politics whereas poorer people tend to say that they do not. 
---


---
## Agents of Socialization
### The School

The school is responsible for teaching kids the rules of democracy and American politics more generally

Elementary school students are taught about the flag and many say the pledge of allegiance 

In middle and high school, students are taught American history and learn about the basics of American government. 

High schoolers often hold mock election for things like student council 

Some universities require students to take an intro to American Gov. class

???
Your socialization isn't limited to your nuclear family. Alongside your parents and siblings when you go to school not only are you learning stuff and then improving your marketibility to get fruitful employment but you are also learning about policy alternative and opposing view points that help soften or harden your own political identity
---

---
## Agents of Socialization 
### Peers 

While schools and families are a major factor in political socialization, so are your friends and co-workers

This reaches well into adulthood

???
While our families are often alike in some respects they also all have their quirks that affect who we are. Interaction with those that are very different or largely the same also impacts our political socialization. For the most part we can think of our parents and upbringing as primary agents

Primacy effect – what is learned first tends to leave a strong and lasting impression that remains with a person throughout life
	Only ten % of young voters vote differently from their parents if their parents both had the same party id

Primary agents – think face to face interaction
	Those agents of socialization that are closest to you. They are often the most influential in an individual’s formative years.

Secondary agents – groups that people are exposed to later in life
	Those agents of socialization that have less intimate relations with an individual. They are often most important in an individual’s later life.

---